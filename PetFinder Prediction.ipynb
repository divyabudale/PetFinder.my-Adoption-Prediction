{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 128\n",
    "pd.options.display.max_columns = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/divya/Downloads/PetFinder/train/train.csv')\n",
    "test  = pd.read_csv('C:/Users/divya/Downloads/PetFinder/test/test.csv')\n",
    "sample_submission = pd.read_csv('C:/Users/divya/Downloads/PetFinder/test/sample_submission.csv')\n",
    "labels_breed = pd.read_csv('C:/Users/divya/Downloads/PetFinder/breed_labels.csv')\n",
    "labels_color= pd.read_csv('C:/Users/divya/Downloads/PetFinder/color_labels.csv')\n",
    "labels_state = pd.read_csv('C:/Users/divya/Downloads/PetFinder/state_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18941, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats = pd.read_csv(\"C:/Users/divya/Downloads/PetFinder/train_img_features.csv\")\n",
    "test_feats = pd.read_csv(\"C:/Users/divya/Downloads/PetFinder/test_img_features.csv\")\n",
    "\n",
    "train_feats.rename({'Unnamed: 0': 'PetID'}, axis='columns', inplace=True)\n",
    "test_feats.rename({'Unnamed: 0': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "train_feats = train_feats.set_index('PetID')\n",
    "test_feats = test_feats.set_index('PetID')\n",
    "\n",
    "train_feats.columns = [f'pic_{i}' for i in range(train_feats.shape[1])]\n",
    "test_feats.columns = [f'pic_{i}' for i in range(test_feats.shape[1])]\n",
    "\n",
    "all_ids = pd.concat([train, test], axis=0, ignore_index=True, sort=False)[['PetID']]\n",
    "all_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "\n",
    "n_components = 32\n",
    "svd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n",
    "\n",
    "features_df = pd.concat([train_feats, test_feats], axis=0)\n",
    "features = features_df[[f'pic_{i}' for i in range(256)]].values\n",
    "\n",
    "svd_col = svd_.fit_transform(features)\n",
    "svd_col = pd.DataFrame(svd_col)\n",
    "svd_col = svd_col.add_prefix('IMG_SVD_')\n",
    "\n",
    "img_features = pd.concat([all_ids, svd_col], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "mag = []\n",
    "score = []\n",
    "ent = []\n",
    "nf_count = 0\n",
    "for pet in train.PetID.values:\n",
    "    try:\n",
    "        with open('C:/Users/divya/Downloads/PetFinder/train_sentiment/' + pet + '.json', 'r',encoding=\"utf8\") as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        \n",
    "        entities = ([x['name'] for x in sentiment['entities']])\n",
    "        sentence_sep = ' '\n",
    "        file_entities = sentence_sep.join(entities)\n",
    "        ent.append(file_entities)\n",
    "        \n",
    "        sent_data = [x['sentiment'] for x in sentiment['sentences']]\n",
    "        sent_data = pd.DataFrame.from_dict(sent_data, orient='columns').sum()\n",
    "        mag.append(sent_data['magnitude'])\n",
    "        score.append(sent_data['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "        ent.append('None')\n",
    "        mag.append(-1)\n",
    "        score.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 6)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = pd.DataFrame(doc_sent_mag,columns={'doc_sent_mag'})\n",
    "df_sentiment['doc_sent_score'] = doc_sent_score\n",
    "df_sentiment['entities'] = ent\n",
    "df_sentiment['sentiment_magnitude'] = mag\n",
    "df_sentiment['sentiment_score'] = score\n",
    "df_sentiment['PetID'] = train.PetID\n",
    "df_sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregates = ['mean', 'sum', 'var']\n",
    "train_sentiment_desc = df_sentiment.groupby(['PetID'])['entities'].unique()\n",
    "train_sentiment_desc = train_sentiment_desc.reset_index()\n",
    "train_sentiment_desc['entities'] = train_sentiment_desc['entities'].apply(lambda x: ' '.join(x))\n",
    "train_sentiment_gr = df_sentiment.drop(['entities'], axis=1)\n",
    "for i in train_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\n",
    "train_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates) \n",
    "train_sentiment_gr.columns = pd.Index(['{}_{}'.format(c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\n",
    "train_sentiment_gr = train_sentiment_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc = train.copy()\n",
    "train_proc = train_proc.merge(train_sentiment_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(train_sentiment_desc, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sent_mag = []\n",
    "doc_sent_score = []\n",
    "mag = []\n",
    "score = []\n",
    "ent = []\n",
    "nf_count = 0\n",
    "for pet in test.PetID.values:\n",
    "    try:\n",
    "        with open('C:/Users/divya/Downloads/PetFinder/test_sentiment/' + pet + '.json', 'r',encoding=\"utf8\") as f:\n",
    "            sentiment = json.load(f)\n",
    "        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "        doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        \n",
    "        entities = ([x['name'] for x in sentiment['entities']])\n",
    "        sentence_sep = ' '\n",
    "        file_entities = sentence_sep.join(entities)\n",
    "        ent.append(file_entities)\n",
    "        \n",
    "        sent_data = [x['sentiment'] for x in sentiment['sentences']]\n",
    "        sent_data = pd.DataFrame.from_dict(sent_data, orient='columns').sum()\n",
    "        mag.append(sent_data['magnitude'])\n",
    "        score.append(sent_data['score'])\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        doc_sent_mag.append(-1)\n",
    "        doc_sent_score.append(-1)\n",
    "        ent.append('None')\n",
    "        mag.append(-1)\n",
    "        score.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3948, 6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment = pd.DataFrame(doc_sent_mag,columns={'doc_sent_mag'})\n",
    "df_sentiment['doc_sent_score'] = doc_sent_score\n",
    "df_sentiment['entities'] = ent\n",
    "df_sentiment['sentiment_magnitude'] = mag\n",
    "df_sentiment['sentiment_score'] = score\n",
    "df_sentiment['PetID'] = test.PetID\n",
    "df_sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregates = ['mean', 'sum', 'avg']\n",
    "test_sentiment_desc = df_sentiment.groupby(['PetID'])['entities'].unique()\n",
    "test_sentiment_desc = test_sentiment_desc.reset_index()\n",
    "test_sentiment_desc['entities'] = test_sentiment_desc['entities'].apply(lambda x: ' '.join(x))\n",
    "test_sentiment_gr = df_sentiment.drop(['entities'], axis=1)\n",
    "for i in test_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\n",
    "test_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)      \n",
    "test_sentiment_gr.columns = pd.Index(['{}_{}'.format(c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\n",
    "test_sentiment_gr = test_sentiment_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proc = test.copy()\n",
    "test_proc = test_proc.merge(test_sentiment_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(test_sentiment_desc, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 24)\n",
      "(14993, 37)\n",
      "(3948, 23)\n",
      "(3948, 36)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(train_proc.shape)\n",
    "print(test.shape)\n",
    "print(test_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "for pet in train.PetID.values:\n",
    "    try:\n",
    "        #with open('../input/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "        with open('C:/Users/divya/Downloads/PetFinder/train_metadata/' + pet + '-1.json', 'r',encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 12)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata = pd.DataFrame(vertex_xs,columns={'vertex_x'})\n",
    "df_metadata['vertex_y'] = vertex_ys\n",
    "df_metadata['bounding_confidence'] = bounding_confidences\n",
    "df_metadata['bounding_importance'] = bounding_importance_fracs\n",
    "df_metadata['dominant_blue'] = dominant_blues\n",
    "df_metadata['dominant_green'] = dominant_greens\n",
    "df_metadata['dominant_red'] = dominant_reds\n",
    "df_metadata['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "df_metadata['dominant_score'] = dominant_scores\n",
    "df_metadata['label_description'] = label_descriptions\n",
    "df_metadata['label_score'] = label_scores\n",
    "df_metadata['PetID'] = train.PetID\n",
    "df_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregates = ['mean', 'sum']\n",
    "train_metadata_desc = df_metadata.groupby(['PetID'])['label_description'].unique()\n",
    "train_metadata_desc = train_metadata_desc.reset_index()\n",
    "train_metadata_desc['label_description'] = train_metadata_desc['label_description'].apply(lambda x : ''.join(x))\n",
    "train_metadata_gr = df_metadata.drop(['label_description'], axis=1)\n",
    "for i in train_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\n",
    "train_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_metadata_gr.columns = pd.Index(['{}_{}'.format(c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\n",
    "train_metadata_gr = train_metadata_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proc = train_proc.merge(train_metadata_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(train_metadata_desc, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_xs = []\n",
    "vertex_ys = []\n",
    "bounding_confidences = []\n",
    "bounding_importance_fracs = []\n",
    "dominant_blues = []\n",
    "dominant_greens = []\n",
    "dominant_reds = []\n",
    "dominant_pixel_fracs = []\n",
    "dominant_scores = []\n",
    "label_descriptions = []\n",
    "label_scores = []\n",
    "nf_count = 0\n",
    "nl_count = 0\n",
    "for pet in test.PetID.values:\n",
    "    try:\n",
    "        #with open('../input/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "        with open('C:/Users/divya/Downloads/PetFinder/test_metadata/' + pet + '-1.json', 'r',encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "    except FileNotFoundError:\n",
    "        nf_count += 1\n",
    "        vertex_xs.append(-1)\n",
    "        vertex_ys.append(-1)\n",
    "        bounding_confidences.append(-1)\n",
    "        bounding_importance_fracs.append(-1)\n",
    "        dominant_blues.append(-1)\n",
    "        dominant_greens.append(-1)\n",
    "        dominant_reds.append(-1)\n",
    "        dominant_pixel_fracs.append(-1)\n",
    "        dominant_scores.append(-1)\n",
    "        label_descriptions.append('nothing')\n",
    "        label_scores.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3948, 12)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata = pd.DataFrame(vertex_xs,columns={'vertex_x'})\n",
    "df_metadata['vertex_y'] = vertex_ys\n",
    "df_metadata['bounding_confidence'] = bounding_confidences\n",
    "df_metadata['bounding_importance'] = bounding_importance_fracs\n",
    "df_metadata['dominant_blue'] = dominant_blues\n",
    "df_metadata['dominant_green'] = dominant_greens\n",
    "df_metadata['dominant_red'] = dominant_reds\n",
    "df_metadata['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "df_metadata['dominant_score'] = dominant_scores\n",
    "df_metadata['label_description'] = label_descriptions\n",
    "df_metadata['label_score'] = label_scores\n",
    "df_metadata['PetID'] = test.PetID\n",
    "df_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregates = ['mean', 'sum', 'var']\n",
    "test_metadata_desc = df_metadata.groupby(['PetID'])['label_description'].unique()\n",
    "test_metadata_desc = test_metadata_desc.reset_index()\n",
    "test_metadata_desc['label_description'] = test_metadata_desc['label_description'].apply(lambda x : ''.join(x))\n",
    "test_metadata_gr = df_metadata.drop(['label_description'], axis=1)\n",
    "for i in test_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\n",
    "test_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_metadata_gr.columns = pd.Index(['{}_{}'.format(c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\n",
    "test_metadata_gr = test_metadata_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proc = test_proc.merge(test_metadata_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(test_metadata_desc, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_breed_main = train_proc[['Breed1']].merge(labels_breed, how='left', left_on='Breed1', right_on='BreedID',suffixes=('', '_main_breed')    )\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "train_breed_second = train_proc[['Breed2']].merge(labels_breed, how='left', left_on='Breed2', right_on='BreedID',suffixes=('', '_second_breed'))\n",
    "\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "train_proc = pd.concat([train_proc, train_breed_main, train_breed_second], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_breed_main = test_proc[['Breed1']].merge(labels_breed, how='left', left_on='Breed1', right_on='BreedID',suffixes=('', '_main_breed')    )\n",
    "test_breed_main = test_breed_main.iloc[:, 2:]\n",
    "test_breed_main = test_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "test_breed_second = test_proc[['Breed2']].merge(labels_breed, how='left', left_on='Breed2', right_on='BreedID',suffixes=('', '_second_breed'))\n",
    "\n",
    "test_breed_second = test_breed_second.iloc[:, 2:]\n",
    "test_breed_second = test_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "test_proc = pd.concat([test_proc, test_breed_main, test_breed_second], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = train_proc.copy()\n",
    "dtest = test_proc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/divya/Downloads/PetFinder/rating.json', 'r',encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "species_keys = list(data.keys())    \n",
    "\n",
    "# Spread species dictionary into a single dictionary.\n",
    "breed_ratings_dict = {**data['cat_breeds']}\n",
    "breed_ratings_dict = {**breed_ratings_dict,**data['dog_breeds']}\n",
    "\n",
    "# Convert into a dataframe, transpose the table so the Breed is the index, rename the index.\n",
    "breed_score_df = pd.DataFrame(breed_ratings_dict).T.reset_index().rename(columns={'index':'BreedName'})\n",
    "train_proc = train_proc.merge(breed_score_df,how='left',left_on='main_breed_BreedName',right_on='BreedName')\n",
    "test_proc = test_proc.merge(breed_score_df,how='left',left_on='main_breed_BreedName',right_on='BreedName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 24)\n",
      "(14993, 110)\n",
      "(3948, 23)\n",
      "(3948, 109)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(train_proc.shape)\n",
    "print(test.shape)\n",
    "print(test_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN structure:\n",
      "Type                                   0\n",
      "Name                                1560\n",
      "Age                                    0\n",
      "Breed1                                 0\n",
      "Breed2                                 0\n",
      "Gender                                 0\n",
      "Color1                                 0\n",
      "Color2                                 0\n",
      "Color3                                 0\n",
      "MaturitySize                           0\n",
      "FurLength                              0\n",
      "Vaccinated                             0\n",
      "Dewormed                               0\n",
      "Sterilized                             0\n",
      "Health                                 0\n",
      "Quantity                               0\n",
      "Fee                                    0\n",
      "State                                  0\n",
      "RescuerID                              0\n",
      "VideoAmt                               0\n",
      "Description                           14\n",
      "PetID                                  0\n",
      "PhotoAmt                               0\n",
      "AdoptionSpeed                       3948\n",
      "doc_sent_mag_MEAN                      0\n",
      "doc_sent_mag_SUM                       0\n",
      "doc_sent_mag_VAR                   18941\n",
      "doc_sent_score_MEAN                    0\n",
      "doc_sent_score_SUM                     0\n",
      "doc_sent_score_VAR                 18941\n",
      "sentiment_magnitude_MEAN               0\n",
      "sentiment_magnitude_SUM                0\n",
      "sentiment_magnitude_VAR            18941\n",
      "sentiment_score_MEAN                   0\n",
      "sentiment_score_SUM                    0\n",
      "sentiment_score_VAR                18941\n",
      "entities                               0\n",
      "vertex_x_MEAN                          0\n",
      "vertex_x_SUM                           0\n",
      "vertex_x_VAR                       18941\n",
      "vertex_y_MEAN                          0\n",
      "vertex_y_SUM                           0\n",
      "vertex_y_VAR                       18941\n",
      "bounding_confidence_MEAN               0\n",
      "bounding_confidence_SUM                0\n",
      "bounding_confidence_VAR            18941\n",
      "bounding_importance_MEAN               0\n",
      "bounding_importance_SUM                0\n",
      "bounding_importance_VAR            18941\n",
      "dominant_blue_MEAN                     0\n",
      "dominant_blue_SUM                      0\n",
      "dominant_blue_VAR                  18941\n",
      "dominant_green_MEAN                    0\n",
      "dominant_green_SUM                     0\n",
      "dominant_green_VAR                 18941\n",
      "dominant_red_MEAN                      0\n",
      "dominant_red_SUM                       0\n",
      "dominant_red_VAR                   18941\n",
      "dominant_pixel_frac_MEAN               0\n",
      "dominant_pixel_frac_SUM                0\n",
      "dominant_pixel_frac_VAR            18941\n",
      "dominant_score_MEAN                    0\n",
      "dominant_score_SUM                     0\n",
      "dominant_score_VAR                 18941\n",
      "label_score_MEAN                       0\n",
      "label_score_SUM                        0\n",
      "label_score_VAR                    18941\n",
      "label_description                      0\n",
      "main_breed_Type                        5\n",
      "main_breed_BreedName                   5\n",
      "second_breed_Type                  13840\n",
      "second_breed_BreedName             13840\n",
      "BreedName                          16067\n",
      " Adaptability                      16815\n",
      " All Around Friendliness           16815\n",
      " Exercise Needs                    16815\n",
      " Health Grooming                   16815\n",
      " Trainability                      16815\n",
      "Adapts Well to Apartment Living    16815\n",
      "Affectionate with Family           16067\n",
      "Amount Of Shedding                 16815\n",
      "Amount of Shedding                 18193\n",
      "Dog Friendly                       16815\n",
      "Drooling Potential                 16815\n",
      "Easy To Groom                      16815\n",
      "Easy To Train                      16815\n",
      "Easy to Groom                      18467\n",
      "Energy Level                       16815\n",
      "Exercise Needs                     16815\n",
      "Friendly Toward Strangers          16120\n",
      "General Health                     16067\n",
      "Good For Novice Owners             16815\n",
      "Incredibly Kid Friendly Dogs       16815\n",
      "Intelligence                       16478\n",
      "Intensity                          16815\n",
      "Kid Friendly                       18193\n",
      "Pet Friendly                       18237\n",
      "Potential For Mouthiness           16815\n",
      "Potential For Playfulness          16815\n",
      "Potential For Weight Gain          16815\n",
      "Potential for Playfulness          18204\n",
      "Prey Drive                         16815\n",
      "Sensitivity Level                  16815\n",
      "Size                               16815\n",
      "Tendency To Bark Or Howl           16815\n",
      "Tendency to Vocalize               18448\n",
      "Tolerates Being Alone              16815\n",
      "Tolerates Cold Weather             16815\n",
      "Tolerates Hot Weather              16815\n",
      "Wanderlust Potential               16815\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\n",
    "print('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = X.dtypes\n",
    "int_cols = column_types[column_types == 'int64']\n",
    "float_cols = column_types[column_types == 'float']\n",
    "cat_cols = column_types[column_types == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original X DF for easier experimentation,\n",
    "# all feature engineering will be performed on this one:\n",
    "X_temp = X.copy()\n",
    "\n",
    "\n",
    "# Select subsets of columns:\n",
    "text_columns = ['Description', 'entities', 'label_description']\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "\n",
    "# Names are all unique, so they can be dropped by default\n",
    "# Same goes for PetID, it shouldn't be used as a feature\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID','BreedName']\n",
    "# RescuerID will also be dropped, as a feature based on this column will be extracted independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count RescuerID occurrences:\n",
    "rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "# Merge as another feature onto main DF:\n",
    "X_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical columns:\n",
    "for i in categorical_columns:\n",
    "    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divya\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Subset text features:\n",
    "X_text = X_temp[text_columns]\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Description          0\n",
       "entities             0\n",
       "label_description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_text.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: Description\n",
      "generating features from: entities\n",
      "generating features from: label_description\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "\n",
    "n_components = 10\n",
    "text_features = []\n",
    "\n",
    "# Generate text features:\n",
    "for i in X_text.columns:\n",
    "    # Initialize decomposition methods:\n",
    "    print('generating features from: {}'.format(i))\n",
    "    svd_ = TruncatedSVD(n_components=n_components, random_state=1337)\n",
    "    nmf_ = NMF(n_components=n_components, random_state=1337)\n",
    "    \n",
    "    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n",
    "    \n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col)\n",
    "    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n",
    "    \n",
    "    text_features.append(svd_col)\n",
    "    text_features.append(nmf_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all extracted features:\n",
    "text_features = pd.concat(text_features, axis=1)\n",
    "\n",
    "# Concatenate with main DF:\n",
    "X_temp = pd.concat([X_temp, text_features], axis=1)\n",
    "\n",
    "# Remove raw text columns:\n",
    "for i in X_text.columns:\n",
    "    X_temp = X_temp.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = X_temp.merge(img_features, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSize(filename):\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "def getDimensions(filename):\n",
    "    img_size = Image.open(filename).size\n",
    "    return img_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_char = '/'\n",
    "\n",
    "from PIL import Image\n",
    "train_df_ids = train[['PetID']]\n",
    "test_df_ids = test[['PetID']]\n",
    "\n",
    "train_image_files = sorted(glob.glob('C:/Users/divya/Downloads/PetFinder/train_images/*.jpg'))\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename']\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n",
    "train_imgs_pets = train_imgs_pets.str.slice(start=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_files = sorted(glob.glob('C:/Users/divya/Downloads/PetFinder/test_images/*.jpg'))\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n",
    "test_imgs_pets = test_imgs_pets.str.slice(start=12)\n",
    "\n",
    "train_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\n",
    "test_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\n",
    "train_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\n",
    "train_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\n",
    "train_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\n",
    "train_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n",
    "\n",
    "test_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\n",
    "test_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\n",
    "test_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\n",
    "test_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\n",
    "test_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {\n",
    "    'image_size': ['sum', 'mean', 'var'],\n",
    "    'width': ['sum', 'mean', 'var'],\n",
    "    'height': ['sum', 'mean', 'var'],\n",
    "}\n",
    "\n",
    "agg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\n",
    "new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "agg_train_imgs.columns = new_columns\n",
    "agg_train_imgs = agg_train_imgs.reset_index()\n",
    "\n",
    "agg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\n",
    "new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "agg_test_imgs.columns = new_columns\n",
    "agg_test_imgs = agg_test_imgs.reset_index()\n",
    "\n",
    "agg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = X_temp.merge(agg_imgs, how='left', on='PetID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (18941, 205)\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns:\n",
    "X_temp = X_temp.drop(to_drop_columns, axis=1)\n",
    "\n",
    "# Check final df shape:\n",
    "print('X shape: {}'.format(X_temp.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14993, 205)\n",
      "X_test shape: (3948, 204)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test again:\n",
    "X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "X_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "\n",
    "# Remove missing target column from test:\n",
    "X_test = X_test.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "\n",
    "# Check if columns between the two DFs are the same:\n",
    "train_cols = X_train.columns.tolist()\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "test_cols = X_test.columns.tolist()\n",
    "\n",
    "assert np.all(train_cols == test_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(-1)\n",
    "X_test = X_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                        0\n",
       "Age                         0\n",
       "Breed1                      0\n",
       "Breed2                      0\n",
       "Gender                      0\n",
       "Color1                      0\n",
       "Color2                      0\n",
       "Color3                      0\n",
       "MaturitySize                0\n",
       "FurLength                   0\n",
       "Vaccinated                  0\n",
       "Dewormed                    0\n",
       "Sterilized                  0\n",
       "Health                      0\n",
       "Quantity                    0\n",
       "Fee                         0\n",
       "State                       0\n",
       "VideoAmt                    0\n",
       "PhotoAmt                    0\n",
       "AdoptionSpeed               0\n",
       "doc_sent_mag_MEAN           0\n",
       "doc_sent_mag_SUM            0\n",
       "doc_sent_mag_VAR            0\n",
       "doc_sent_score_MEAN         0\n",
       "doc_sent_score_SUM          0\n",
       "doc_sent_score_VAR          0\n",
       "sentiment_magnitude_MEAN    0\n",
       "sentiment_magnitude_SUM     0\n",
       "sentiment_magnitude_VAR     0\n",
       "sentiment_score_MEAN        0\n",
       "sentiment_score_SUM         0\n",
       "sentiment_score_VAR         0\n",
       "vertex_x_MEAN               0\n",
       "vertex_x_SUM                0\n",
       "vertex_x_VAR                0\n",
       "vertex_y_MEAN               0\n",
       "vertex_y_SUM                0\n",
       "vertex_y_VAR                0\n",
       "bounding_confidence_MEAN    0\n",
       "bounding_confidence_SUM     0\n",
       "bounding_confidence_VAR     0\n",
       "bounding_importance_MEAN    0\n",
       "bounding_importance_SUM     0\n",
       "bounding_importance_VAR     0\n",
       "dominant_blue_MEAN          0\n",
       "dominant_blue_SUM           0\n",
       "dominant_blue_VAR           0\n",
       "dominant_green_MEAN         0\n",
       "dominant_green_SUM          0\n",
       "dominant_green_VAR          0\n",
       "dominant_red_MEAN           0\n",
       "dominant_red_SUM            0\n",
       "dominant_red_VAR            0\n",
       "dominant_pixel_frac_MEAN    0\n",
       "dominant_pixel_frac_SUM     0\n",
       "dominant_pixel_frac_VAR     0\n",
       "dominant_score_MEAN         0\n",
       "dominant_score_SUM          0\n",
       "dominant_score_VAR          0\n",
       "label_score_MEAN            0\n",
       "label_score_SUM             0\n",
       "label_score_VAR             0\n",
       "main_breed_Type             0\n",
       "main_breed_BreedName        0\n",
       "                           ..\n",
       "NMF_entities_7              0\n",
       "NMF_entities_8              0\n",
       "NMF_entities_9              0\n",
       "SVD_label_description_0     0\n",
       "SVD_label_description_1     0\n",
       "SVD_label_description_2     0\n",
       "SVD_label_description_3     0\n",
       "SVD_label_description_4     0\n",
       "SVD_label_description_5     0\n",
       "SVD_label_description_6     0\n",
       "SVD_label_description_7     0\n",
       "SVD_label_description_8     0\n",
       "SVD_label_description_9     0\n",
       "NMF_label_description_0     0\n",
       "NMF_label_description_1     0\n",
       "NMF_label_description_2     0\n",
       "NMF_label_description_3     0\n",
       "NMF_label_description_4     0\n",
       "NMF_label_description_5     0\n",
       "NMF_label_description_6     0\n",
       "NMF_label_description_7     0\n",
       "NMF_label_description_8     0\n",
       "NMF_label_description_9     0\n",
       "IMG_SVD_0                   0\n",
       "IMG_SVD_1                   0\n",
       "IMG_SVD_2                   0\n",
       "IMG_SVD_3                   0\n",
       "IMG_SVD_4                   0\n",
       "IMG_SVD_5                   0\n",
       "IMG_SVD_6                   0\n",
       "IMG_SVD_7                   0\n",
       "IMG_SVD_8                   0\n",
       "IMG_SVD_9                   0\n",
       "IMG_SVD_10                  0\n",
       "IMG_SVD_11                  0\n",
       "IMG_SVD_12                  0\n",
       "IMG_SVD_13                  0\n",
       "IMG_SVD_14                  0\n",
       "IMG_SVD_15                  0\n",
       "IMG_SVD_16                  0\n",
       "IMG_SVD_17                  0\n",
       "IMG_SVD_18                  0\n",
       "IMG_SVD_19                  0\n",
       "IMG_SVD_20                  0\n",
       "IMG_SVD_21                  0\n",
       "IMG_SVD_22                  0\n",
       "IMG_SVD_23                  0\n",
       "IMG_SVD_24                  0\n",
       "IMG_SVD_25                  0\n",
       "IMG_SVD_26                  0\n",
       "IMG_SVD_27                  0\n",
       "IMG_SVD_28                  0\n",
       "IMG_SVD_29                  0\n",
       "IMG_SVD_30                  0\n",
       "IMG_SVD_31                  0\n",
       "image_size_sum              0\n",
       "image_size_mean             0\n",
       "image_size_var              0\n",
       "width_sum                   0\n",
       "width_mean                  0\n",
       "width_var                   0\n",
       "height_sum                  0\n",
       "height_mean                 0\n",
       "height_var                  0\n",
       "Length: 205, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pd.isnull(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                        0\n",
       "Age                         0\n",
       "Breed1                      0\n",
       "Breed2                      0\n",
       "Gender                      0\n",
       "Color1                      0\n",
       "Color2                      0\n",
       "Color3                      0\n",
       "MaturitySize                0\n",
       "FurLength                   0\n",
       "Vaccinated                  0\n",
       "Dewormed                    0\n",
       "Sterilized                  0\n",
       "Health                      0\n",
       "Quantity                    0\n",
       "Fee                         0\n",
       "State                       0\n",
       "VideoAmt                    0\n",
       "PhotoAmt                    0\n",
       "doc_sent_mag_MEAN           0\n",
       "doc_sent_mag_SUM            0\n",
       "doc_sent_mag_VAR            0\n",
       "doc_sent_score_MEAN         0\n",
       "doc_sent_score_SUM          0\n",
       "doc_sent_score_VAR          0\n",
       "sentiment_magnitude_MEAN    0\n",
       "sentiment_magnitude_SUM     0\n",
       "sentiment_magnitude_VAR     0\n",
       "sentiment_score_MEAN        0\n",
       "sentiment_score_SUM         0\n",
       "sentiment_score_VAR         0\n",
       "vertex_x_MEAN               0\n",
       "vertex_x_SUM                0\n",
       "vertex_x_VAR                0\n",
       "vertex_y_MEAN               0\n",
       "vertex_y_SUM                0\n",
       "vertex_y_VAR                0\n",
       "bounding_confidence_MEAN    0\n",
       "bounding_confidence_SUM     0\n",
       "bounding_confidence_VAR     0\n",
       "bounding_importance_MEAN    0\n",
       "bounding_importance_SUM     0\n",
       "bounding_importance_VAR     0\n",
       "dominant_blue_MEAN          0\n",
       "dominant_blue_SUM           0\n",
       "dominant_blue_VAR           0\n",
       "dominant_green_MEAN         0\n",
       "dominant_green_SUM          0\n",
       "dominant_green_VAR          0\n",
       "dominant_red_MEAN           0\n",
       "dominant_red_SUM            0\n",
       "dominant_red_VAR            0\n",
       "dominant_pixel_frac_MEAN    0\n",
       "dominant_pixel_frac_SUM     0\n",
       "dominant_pixel_frac_VAR     0\n",
       "dominant_score_MEAN         0\n",
       "dominant_score_SUM          0\n",
       "dominant_score_VAR          0\n",
       "label_score_MEAN            0\n",
       "label_score_SUM             0\n",
       "label_score_VAR             0\n",
       "main_breed_Type             0\n",
       "main_breed_BreedName        0\n",
       "second_breed_Type           0\n",
       "                           ..\n",
       "NMF_entities_7              0\n",
       "NMF_entities_8              0\n",
       "NMF_entities_9              0\n",
       "SVD_label_description_0     0\n",
       "SVD_label_description_1     0\n",
       "SVD_label_description_2     0\n",
       "SVD_label_description_3     0\n",
       "SVD_label_description_4     0\n",
       "SVD_label_description_5     0\n",
       "SVD_label_description_6     0\n",
       "SVD_label_description_7     0\n",
       "SVD_label_description_8     0\n",
       "SVD_label_description_9     0\n",
       "NMF_label_description_0     0\n",
       "NMF_label_description_1     0\n",
       "NMF_label_description_2     0\n",
       "NMF_label_description_3     0\n",
       "NMF_label_description_4     0\n",
       "NMF_label_description_5     0\n",
       "NMF_label_description_6     0\n",
       "NMF_label_description_7     0\n",
       "NMF_label_description_8     0\n",
       "NMF_label_description_9     0\n",
       "IMG_SVD_0                   0\n",
       "IMG_SVD_1                   0\n",
       "IMG_SVD_2                   0\n",
       "IMG_SVD_3                   0\n",
       "IMG_SVD_4                   0\n",
       "IMG_SVD_5                   0\n",
       "IMG_SVD_6                   0\n",
       "IMG_SVD_7                   0\n",
       "IMG_SVD_8                   0\n",
       "IMG_SVD_9                   0\n",
       "IMG_SVD_10                  0\n",
       "IMG_SVD_11                  0\n",
       "IMG_SVD_12                  0\n",
       "IMG_SVD_13                  0\n",
       "IMG_SVD_14                  0\n",
       "IMG_SVD_15                  0\n",
       "IMG_SVD_16                  0\n",
       "IMG_SVD_17                  0\n",
       "IMG_SVD_18                  0\n",
       "IMG_SVD_19                  0\n",
       "IMG_SVD_20                  0\n",
       "IMG_SVD_21                  0\n",
       "IMG_SVD_22                  0\n",
       "IMG_SVD_23                  0\n",
       "IMG_SVD_24                  0\n",
       "IMG_SVD_25                  0\n",
       "IMG_SVD_26                  0\n",
       "IMG_SVD_27                  0\n",
       "IMG_SVD_28                  0\n",
       "IMG_SVD_29                  0\n",
       "IMG_SVD_30                  0\n",
       "IMG_SVD_31                  0\n",
       "image_size_sum              0\n",
       "image_size_mean             0\n",
       "image_size_var              0\n",
       "width_sum                   0\n",
       "width_mean                  0\n",
       "width_var                   0\n",
       "height_sum                  0\n",
       "height_mean                 0\n",
       "height_var                  0\n",
       "Length: 204, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pd.isnull(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n",
    "\n",
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9, #12\n",
    "          'learning_rate': 0.01, #0.001\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17}\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 500\n",
    "verbose_eval = 100\n",
    "num_rounds = 10000\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3229, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.04674\tvalid_1's rmse: 1.0924\n",
      "[200]\ttraining's rmse: 0.983136\tvalid_1's rmse: 1.06672\n",
      "[300]\ttraining's rmse: 0.940831\tvalid_1's rmse: 1.05582\n",
      "[400]\ttraining's rmse: 0.910008\tvalid_1's rmse: 1.04744\n",
      "[500]\ttraining's rmse: 0.888358\tvalid_1's rmse: 1.04311\n",
      "[600]\ttraining's rmse: 0.867172\tvalid_1's rmse: 1.04044\n",
      "[700]\ttraining's rmse: 0.849259\tvalid_1's rmse: 1.0389\n",
      "[800]\ttraining's rmse: 0.831658\tvalid_1's rmse: 1.03677\n",
      "[900]\ttraining's rmse: 0.814414\tvalid_1's rmse: 1.03553\n",
      "[1000]\ttraining's rmse: 0.797105\tvalid_1's rmse: 1.0343\n",
      "[1100]\ttraining's rmse: 0.780999\tvalid_1's rmse: 1.0333\n",
      "[1200]\ttraining's rmse: 0.764379\tvalid_1's rmse: 1.03215\n",
      "[1300]\ttraining's rmse: 0.748356\tvalid_1's rmse: 1.03109\n",
      "[1400]\ttraining's rmse: 0.734215\tvalid_1's rmse: 1.0306\n",
      "[1500]\ttraining's rmse: 0.719405\tvalid_1's rmse: 1.03014\n",
      "[1600]\ttraining's rmse: 0.705638\tvalid_1's rmse: 1.02994\n",
      "[1700]\ttraining's rmse: 0.693102\tvalid_1's rmse: 1.02965\n",
      "[1800]\ttraining's rmse: 0.681455\tvalid_1's rmse: 1.02942\n",
      "[1900]\ttraining's rmse: 0.669796\tvalid_1's rmse: 1.02947\n",
      "[2000]\ttraining's rmse: 0.657878\tvalid_1's rmse: 1.02944\n",
      "[2100]\ttraining's rmse: 0.645989\tvalid_1's rmse: 1.02905\n",
      "[2200]\ttraining's rmse: 0.633442\tvalid_1's rmse: 1.02892\n",
      "[2300]\ttraining's rmse: 0.62152\tvalid_1's rmse: 1.02872\n",
      "[2400]\ttraining's rmse: 0.608741\tvalid_1's rmse: 1.02831\n",
      "[2500]\ttraining's rmse: 0.597456\tvalid_1's rmse: 1.02814\n",
      "[2600]\ttraining's rmse: 0.589378\tvalid_1's rmse: 1.0281\n",
      "[2700]\ttraining's rmse: 0.579291\tvalid_1's rmse: 1.02822\n",
      "[2800]\ttraining's rmse: 0.569227\tvalid_1's rmse: 1.02854\n",
      "[2900]\ttraining's rmse: 0.558861\tvalid_1's rmse: 1.02887\n",
      "[3000]\ttraining's rmse: 0.548181\tvalid_1's rmse: 1.02877\n",
      "Early stopping, best iteration is:\n",
      "[2554]\ttraining's rmse: 0.593124\tvalid_1's rmse: 1.02799\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3229, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.0462\tvalid_1's rmse: 1.0916\n",
      "[200]\ttraining's rmse: 0.981942\tvalid_1's rmse: 1.06848\n",
      "[300]\ttraining's rmse: 0.939446\tvalid_1's rmse: 1.05816\n",
      "[400]\ttraining's rmse: 0.908846\tvalid_1's rmse: 1.05316\n",
      "[500]\ttraining's rmse: 0.884855\tvalid_1's rmse: 1.05073\n",
      "[600]\ttraining's rmse: 0.866175\tvalid_1's rmse: 1.04937\n",
      "[700]\ttraining's rmse: 0.846961\tvalid_1's rmse: 1.04851\n",
      "[800]\ttraining's rmse: 0.828865\tvalid_1's rmse: 1.04743\n",
      "[900]\ttraining's rmse: 0.811643\tvalid_1's rmse: 1.04638\n",
      "[1000]\ttraining's rmse: 0.796523\tvalid_1's rmse: 1.04581\n",
      "[1100]\ttraining's rmse: 0.781787\tvalid_1's rmse: 1.04568\n",
      "[1200]\ttraining's rmse: 0.766669\tvalid_1's rmse: 1.046\n",
      "[1300]\ttraining's rmse: 0.751388\tvalid_1's rmse: 1.04589\n",
      "[1400]\ttraining's rmse: 0.737634\tvalid_1's rmse: 1.04556\n",
      "[1500]\ttraining's rmse: 0.725424\tvalid_1's rmse: 1.04577\n",
      "[1600]\ttraining's rmse: 0.713247\tvalid_1's rmse: 1.04581\n",
      "[1700]\ttraining's rmse: 0.699355\tvalid_1's rmse: 1.04583\n",
      "[1800]\ttraining's rmse: 0.687567\tvalid_1's rmse: 1.04607\n",
      "Early stopping, best iteration is:\n",
      "[1376]\ttraining's rmse: 0.740967\tvalid_1's rmse: 1.04553\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.04792\tvalid_1's rmse: 1.08651\n",
      "[200]\ttraining's rmse: 0.986211\tvalid_1's rmse: 1.05947\n",
      "[300]\ttraining's rmse: 0.946087\tvalid_1's rmse: 1.04824\n",
      "[400]\ttraining's rmse: 0.917705\tvalid_1's rmse: 1.0431\n",
      "[500]\ttraining's rmse: 0.892027\tvalid_1's rmse: 1.03985\n",
      "[600]\ttraining's rmse: 0.873137\tvalid_1's rmse: 1.03795\n",
      "[700]\ttraining's rmse: 0.854362\tvalid_1's rmse: 1.03597\n",
      "[800]\ttraining's rmse: 0.837935\tvalid_1's rmse: 1.03496\n",
      "[900]\ttraining's rmse: 0.823136\tvalid_1's rmse: 1.03399\n",
      "[1000]\ttraining's rmse: 0.808337\tvalid_1's rmse: 1.03336\n",
      "[1100]\ttraining's rmse: 0.792351\tvalid_1's rmse: 1.03274\n",
      "[1200]\ttraining's rmse: 0.77746\tvalid_1's rmse: 1.03232\n",
      "[1300]\ttraining's rmse: 0.763398\tvalid_1's rmse: 1.03156\n",
      "[1400]\ttraining's rmse: 0.749556\tvalid_1's rmse: 1.03129\n",
      "[1500]\ttraining's rmse: 0.736249\tvalid_1's rmse: 1.03154\n",
      "[1600]\ttraining's rmse: 0.723918\tvalid_1's rmse: 1.0314\n",
      "[1700]\ttraining's rmse: 0.711433\tvalid_1's rmse: 1.03131\n",
      "[1800]\ttraining's rmse: 0.697507\tvalid_1's rmse: 1.03128\n",
      "[1900]\ttraining's rmse: 0.685181\tvalid_1's rmse: 1.03154\n",
      "[2000]\ttraining's rmse: 0.673999\tvalid_1's rmse: 1.03183\n",
      "[2100]\ttraining's rmse: 0.661332\tvalid_1's rmse: 1.03172\n",
      "[2200]\ttraining's rmse: 0.649542\tvalid_1's rmse: 1.03157\n",
      "Early stopping, best iteration is:\n",
      "[1745]\ttraining's rmse: 0.705139\tvalid_1's rmse: 1.0311\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.04765\tvalid_1's rmse: 1.08583\n",
      "[200]\ttraining's rmse: 0.984621\tvalid_1's rmse: 1.05932\n",
      "[300]\ttraining's rmse: 0.945057\tvalid_1's rmse: 1.04674\n",
      "[400]\ttraining's rmse: 0.916385\tvalid_1's rmse: 1.03982\n",
      "[500]\ttraining's rmse: 0.8923\tvalid_1's rmse: 1.03618\n",
      "[600]\ttraining's rmse: 0.87235\tvalid_1's rmse: 1.03406\n",
      "[700]\ttraining's rmse: 0.854702\tvalid_1's rmse: 1.03195\n",
      "[800]\ttraining's rmse: 0.837279\tvalid_1's rmse: 1.0304\n",
      "[900]\ttraining's rmse: 0.820742\tvalid_1's rmse: 1.02927\n",
      "[1000]\ttraining's rmse: 0.805172\tvalid_1's rmse: 1.02826\n",
      "[1100]\ttraining's rmse: 0.78897\tvalid_1's rmse: 1.02733\n",
      "[1200]\ttraining's rmse: 0.771822\tvalid_1's rmse: 1.02643\n",
      "[1300]\ttraining's rmse: 0.754582\tvalid_1's rmse: 1.02556\n",
      "[1400]\ttraining's rmse: 0.73797\tvalid_1's rmse: 1.02489\n",
      "[1500]\ttraining's rmse: 0.723446\tvalid_1's rmse: 1.02449\n",
      "[1600]\ttraining's rmse: 0.7098\tvalid_1's rmse: 1.0236\n",
      "[1700]\ttraining's rmse: 0.696291\tvalid_1's rmse: 1.02367\n",
      "[1800]\ttraining's rmse: 0.683439\tvalid_1's rmse: 1.0234\n",
      "[1900]\ttraining's rmse: 0.671859\tvalid_1's rmse: 1.02305\n",
      "[2000]\ttraining's rmse: 0.65997\tvalid_1's rmse: 1.02277\n",
      "[2100]\ttraining's rmse: 0.648015\tvalid_1's rmse: 1.02292\n",
      "[2200]\ttraining's rmse: 0.636386\tvalid_1's rmse: 1.0227\n",
      "[2300]\ttraining's rmse: 0.623886\tvalid_1's rmse: 1.02286\n",
      "[2400]\ttraining's rmse: 0.611888\tvalid_1's rmse: 1.02289\n",
      "[2500]\ttraining's rmse: 0.601048\tvalid_1's rmse: 1.02301\n",
      "[2600]\ttraining's rmse: 0.591102\tvalid_1's rmse: 1.02324\n",
      "Early stopping, best iteration is:\n",
      "[2151]\ttraining's rmse: 0.641675\tvalid_1's rmse: 1.02254\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2608, 1.0: 2472, 0.0: 328})\n",
      "training LGB:\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\ttraining's rmse: 1.04671\tvalid_1's rmse: 1.09293\n",
      "[200]\ttraining's rmse: 0.983499\tvalid_1's rmse: 1.0662\n",
      "[300]\ttraining's rmse: 0.940994\tvalid_1's rmse: 1.05374\n",
      "[400]\ttraining's rmse: 0.91045\tvalid_1's rmse: 1.04781\n",
      "[500]\ttraining's rmse: 0.886227\tvalid_1's rmse: 1.044\n",
      "[600]\ttraining's rmse: 0.86617\tvalid_1's rmse: 1.0416\n",
      "[700]\ttraining's rmse: 0.847653\tvalid_1's rmse: 1.04018\n",
      "[800]\ttraining's rmse: 0.82934\tvalid_1's rmse: 1.03852\n",
      "[900]\ttraining's rmse: 0.812712\tvalid_1's rmse: 1.03771\n",
      "[1000]\ttraining's rmse: 0.797303\tvalid_1's rmse: 1.03704\n",
      "[1100]\ttraining's rmse: 0.780281\tvalid_1's rmse: 1.03633\n",
      "[1200]\ttraining's rmse: 0.765316\tvalid_1's rmse: 1.03606\n",
      "[1300]\ttraining's rmse: 0.749913\tvalid_1's rmse: 1.03543\n",
      "[1400]\ttraining's rmse: 0.734664\tvalid_1's rmse: 1.03514\n",
      "[1500]\ttraining's rmse: 0.721678\tvalid_1's rmse: 1.0347\n",
      "[1600]\ttraining's rmse: 0.708528\tvalid_1's rmse: 1.03439\n",
      "[1700]\ttraining's rmse: 0.695528\tvalid_1's rmse: 1.03465\n",
      "[1800]\ttraining's rmse: 0.682048\tvalid_1's rmse: 1.03489\n",
      "[1900]\ttraining's rmse: 0.669839\tvalid_1's rmse: 1.03528\n",
      "[2000]\ttraining's rmse: 0.656586\tvalid_1's rmse: 1.03573\n",
      "[2100]\ttraining's rmse: 0.644253\tvalid_1's rmse: 1.03515\n",
      "Early stopping, best iteration is:\n",
      "[1600]\ttraining's rmse: 0.708528\tvalid_1's rmse: 1.03439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=1337)\n",
    "\n",
    "\n",
    "oof_train = np.zeros((X_train.shape[0]))\n",
    "oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "    \n",
    "    X_tr = X_train.iloc[train_index, :]\n",
    "    X_val = X_train.iloc[valid_index, :]\n",
    "    \n",
    "    y_tr = X_tr['AdoptionSpeed'].values\n",
    "    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "    \n",
    "    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "    \n",
    "    d_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    \n",
    "    print('training LGB:')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    oof_train[valid_index] = val_pred\n",
    "    oof_test[:, i] = test_pred\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  50.,  382., 1536., 3055., 3476., 2978., 2060., 1091.,  337.,\n",
       "          28.]),\n",
       " array([0.79542823, 1.1542681 , 1.51310796, 1.87194783, 2.23078769,\n",
       "        2.58962755, 2.94846742, 3.30730728, 3.66614715, 4.02498701,\n",
       "        4.38382687]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Counts =  Counter({4.0: 4197, 2.0: 4037, 3.0: 3259, 1.0: 3090, 0.0: 410})\n",
      "Predicted Counts =  Counter({3.0: 3876, 1.0: 3826, 2.0: 3699, 4.0: 3592})\n",
      "Coefficients =  [0.53639425 2.10876392 2.48716831 2.93809559]\n",
      "QWK =  0.4613605294272327\n"
     ]
    }
   ],
   "source": [
    "# Compute QWK based on OOF train predictions:\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(oof_train, X_train['AdoptionSpeed'].values)\n",
    "coefficients = optR.coefficients()\n",
    "pred_test_y_k = optR.predict(oof_train, coefficients)\n",
    "print(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values))\n",
    "print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "print(\"Coefficients = \", coefficients)\n",
    "qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred distribution: Counter({3: 3876, 1: 3826, 2: 3699, 4: 3592})\n",
      "test pred distribution: Counter({3.0: 1033, 1.0: 1024, 4.0: 953, 2.0: 938})\n"
     ]
    }
   ],
   "source": [
    "# Manually adjusted coefficients:\n",
    "\n",
    "coefficients_ = coefficients.copy()\n",
    "\n",
    "coefficients_[0] = 1.645\n",
    "coefficients_[1] = 2.115\n",
    "coefficients_[3] = 2.84\n",
    "\n",
    "train_predictions = optR.predict(oof_train, coefficients).astype(int)\n",
    "print('train pred distribution: {}'.format(Counter(train_predictions)))\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients)\n",
    "print('test pred distribution: {}'.format(Counter(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution inspection of original target and predicted train and test:\n",
    "\n",
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n",
    "print(\"\\nTrain Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "print(\"\\nTest Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_type= \"split\" # \"gain\"\n",
    "idx_sort = np.argsort(model.feature_importance(importance_type=importance_type))[::-1]\n",
    "names_sorted = np.array(model.feature_name())[idx_sort]\n",
    "imports_sorted = model.feature_importance(importance_type=importance_type)[idx_sort]\n",
    "for n, im in zip(names_sorted, imports_sorted):\n",
    "    print(n, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\n",
    "submission.head()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
